{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all $x$, $y$, $\\pi(x | y) P(y) = \\pi(y | x) P(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metropolis-Hastings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Propose $x' ~ \\pi(x' | x)$\n",
    "2. Set $x$ to $x'$ with probability $\\min\\left(1, \\frac{P(x')\\pi(x | x')}{P(x)\\pi(x' | x)} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ergodicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neal RM. Slice Sampling. The Annals of Statistics. 2003;31: 705â€“767.\n",
    "\n",
    "Sample uniformly from the area under the density curve, only keep the horizontal coordinate.\n",
    "\n",
    "1. Sample $y$ from $\\mathbb{U}(0, P(x))$\n",
    "2. Sample $x'$ from union of intervals such that $P(x') > y$\n",
    "\n",
    "If step (2) is difficult, fine to use any proposal that keeps the uniform distribution over that interval union invariant. The target being uniform opens additional options.\n",
    "\n",
    "## Stepping out + shrinkage proposal\n",
    "\n",
    "1. Sample interval w/ width $w$ around $x$\n",
    "2. Step out left/right until both endpoints outside the interval\n",
    "    - Can also double the interval, choose which side randomly\n",
    "3. Sample from the interval until it's inside the slice, shrink upon rejection\n",
    "\n",
    "Important the the candidate states are chosen with the same probability whether you start at $x$ or $x'$.\n",
    "\n",
    "Neal discusses interesting proof strategy:\n",
    "\n",
    "Let's prove a stronger condition: $P(x', r | x) = P(x, f(r) | x')$ where $r$ are the random choices made while generating proposal, and $f$ is a 1:1 mapping w/ Jacobian = 1.\n",
    "\n",
    "Can adapt $w$ if no limit is placed on the width of the interval expansion and distribution is known to be unimodal.\n",
    "\n",
    "Direct N-D expansion possible, either by not expanding at all or using the doubling procedure and testing a random corner. Neal says it works less well than going dimension-by-dimension.\n",
    "\n",
    "## Other proposals\n",
    "\n",
    "- Neal has a cool pseudo-posterior method of generating candidate distributions based on past rejections.\n",
    "- Overrelaxation -> if conditional dist is gaussian, choose samples on the other side of the mode as the current sample\n",
    "- Can use HMC w/ reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NUTS\n",
    "\n",
    "## NUTS and slice sampling\n",
    "\n",
    "In many ways, the original NUTS is literally slice sampling with a particular doubling scheme. It's quite similar in how it picks the doubling direction randomly.\n",
    "\n",
    "The tree construction proceeds as follows:\n",
    "\n",
    "1. Start at state $\\theta_i, m_i$\n",
    "2. Randomly go left or right from the current tree $2^j$ steps, where $j$ is the depth of the tree\n",
    "3. Stop if either diverged, or one of the new states fails the U-turn criterion (grad of ESJD)\n",
    "4. Sample from the set of feasible states using a distribution that preserves uniform distribution over the feasible states\n",
    "\n",
    "It's important to discard states generated by (3) that would not be able to generate the entire tree (for reversibility). This is done by discarding entire output of (3) if any except the left/rightmost node satisfy the stopping conditions. Note that this means that we need to check every subtree... this is only $\\log_2 j$ checks however, but this underlies the memory requirements.\n",
    "\n",
    "Optimizations:\n",
    "\n",
    "1. We can use a different kernel to sample from the feasible state, e.g. some sort of stick breaknig construction? The key point is that we no longer need to store all the feasible set states.\n",
    "2. Can break out early when constructing the final tree doubling as soon as the bad thing is detected.\n",
    "\n",
    "Acceptance probability:\n",
    "\n",
    "Use the average acceptance probability on the final doubling iteration (weird?).\n",
    "\n",
    "## Non-slice interpretation\n",
    "\n",
    "Betancourt suggests just using multinomial sampling. The idea is that we propose a set of states such that this forms a reversible proposal, and then sample from them. Sampling a state from this proposed set merely has to preserve the 'canonical' density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annealed Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background: simulated annealing can be done within MCMC context, but no guarantee that the final samples are from the true distribution.\n",
    "\n",
    "Given a sequence of unnormalized densities $\\pi_j, j \\in [0, n]$ where $\\pi_0$ is the target distribution and a sequence of transition kernels $T_j(x' | x)$ which leaves $\\tilde{\\pi}_j$ invariant. $T_j$ need not be ergodic.\n",
    "\n",
    "Now, generate a sequence of transitions, $x_n \\sim \\pi_n$, $x_{j - 1}\\sim T_j(x_{j - 1} | x_j)$ and then the following weights:\n",
    "\n",
    "$w_j = \\frac{\\pi_{j - 1}(x_{j - 1})}{\\pi_j(x_{j - 1})}$, and the overall weight being $w^{(i)} = \\prod_{j=1}^n w_j$\n",
    "\n",
    "ESS can be computed via normalized weights as:\n",
    "\n",
    "$ESS = \\frac{1}{\\sum_i w_i^2}$\n",
    "\n",
    "This can be derived directly from a weighted sum representation:\n",
    "\n",
    "$ESS = \\frac{\\sigma^2}{var[\\hat{\\theta}]} = \\frac{\\sigma^2}{\\sum w^2 \\sigma^2} = \\frac{1}{\\sum w^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's at least two variants of SMC, unclear how they are related.\n",
    "\n",
    "1. Dynamic: Have a sequence of unnormalized distributions $\\pi_t(x_{1:t})$, $t \\in [1, T]$\n",
    "2. Static: Have a sequence of unnormalized distributions $\\pi_t(x)$, $t \\in [1, T]$ (note how this is reversed from AIS above).\n",
    "\n",
    "## Dynamic\n",
    "\n",
    "Construct a proposal distribution $q_t(x_{1:t}) = q_{t - 1}(x_{1:t-1})q_t(x_t | x_{1:t-1})$. We can now generate particles and weigh them as $w_t(x_{1:t}) =  w_{t - 1}(x_{1:t - 1}) \\tilde{w}_t(x_{1:t})$ where $\\tilde{w}_t(x_{1:t}) = \\frac{\\pi_t(x_{1:t})}{\\pi_{t - 1}(x_{1:t - 1}) q_t(x_t | x_{1:t - 1})}$. The initial weight is a regular IS weight: $w_1 = \\frac{\\pi_1(x_1)}{q_1(x_1)}$.\n",
    "\n",
    "Some examples:\n",
    "\n",
    "Let's say $\\pi_t(x_{1:t}) = \\pi_1(x_1) \\prod \\pi_t(x_t | x_{t - 1})$, then $\\tilde{w}_t(x_{1:t}) = \\frac{\\pi_t(x_t | x_{t - 1})}{q_t(x_t | x_{1:t - 1})}$\n",
    "\n",
    "Let's say $\\pi_t(x_{1:t}) = \\pi_t(x_t) \\prod \\pi_{t-1}(x_{t-1} | x_t)$, then $\\tilde{w}_t(x_{1:t}) = \\frac{\\pi_t(x_t) \\pi_{t - 1}(x_{t - 1} | x_t)}{\\pi_{t - 1}(x_{t - 1}) q_t(x_t | x_{1:t - 1})}$\n",
    "\n",
    "Let's say $\\pi_t(x_{1:t}) = \\pi_1(x_1) \\prod \\pi_t(x_t | x_{t - 1}) \\pi_t(y_t | x_t)$ and we choose $q_t(x_t | x_{1:t - 1}) = \\pi_t(x_t | x_{t - 1})$, then $\\tilde{w}_t(x_{1:t}) = \\pi_t(y_t | x_t)$\n",
    "\n",
    "Resampling: Goal is to discard unpromising particles, trade short term variance for long term. Convergence results show that one effect of this is reducing the effective dimensionality of the problem by resetting the system to only care about $q_t(x_t | x_{1:t-1})$. Because of this, only want to resample when ESS (or some other related criteria) is below a threshold. Don't want to do multinomial, but something more determinstic. Resampling should be done before any operation that does not affect importance weights to minimize the loss of information.\n",
    "\n",
    "MCMC: Can insert MCMC steps (after resampling) that are invariant with respect to $\\pi_t(x_{t - L:t})$ for some lag $L$.\n",
    "\n",
    "## Static\n",
    "\n",
    "We first contrast with (Static? Nomenclature is bad here) Sequential Importance Sampling, where we use a proposal distribution $q_t = \\int dx\\, q_{t - 1} T_t (x' | x)$, where $T$ is some transition kernel (if using MCMC, it'll target $\\pi_t$). The issue is that for reasonable kernels $T$ we cannot compute the importance weights.\n",
    "\n",
    "Instead, expand the state space to $x_{1:T}$ and construct a target distribution $\\pi(x_{1:T}) = \\pi_T \\prod_{t=1}^{T - 1} L_t (x_t | x_{t + 1})$ where $L$ is the backward kernel. Note that now we have converted the problem into a dynamic one, so we can use the dynamic SMC methods to solve it. This isn't free, however, as we're not sampling over a larger space, incurring additional variance. This variance can be mitigated through a good choice of $L_t$, which is a free variable since we only care about the marginal for $t=T$.\n",
    "\n",
    "Starting with a set of particles $w^{(i)}_{t - 1}(x^{(i)}_{1:t - 1})$, $x^{(i)}_{1:t - 1}$ that are targeting $\\pi_{t-1}$, we can advance the particles by sampling from $T_t (x_t | x_{t - 1})$ and incrementing the weights as: $w^{(i)}_{t} = w^{(i)}_{t - 1}(x^{(i)}_{1:t - 1}) \\tilde{w_t}(x_{t - 1}, x_{t})$ where $\\tilde{w_t}(x_{t - 1}, x_{t}) = \\frac{\\pi_{t}(x_{t}) L_{t - 1}(x_{t - 1} | x_{t}) }{\\pi_{t - 1}(x_{t - 1}) T_{t}(x_{t} | x_{t - 1})}$.\n",
    "\n",
    "For AIS, we choose $L_{t - 1}(x_{t - 1} | x_{t}) = \\frac{\\pi_t(x_{t - 1}) T_t(x_{t - 1}, x_t)}{\\pi_t(x_t)}$ which gives $\\tilde{w_t}(x_{t - 1}, x_{t}) = \\frac{\\pi_{t}(x_{t - 1})}{\\pi_{t - 1}(x_{t - 1})}$.\n",
    "\n",
    "For example: let's say $\\pi_{t}(x) = \\pi(x) \\prod \\pi(y_t | x)$, then $\\tilde{w_t} = \\pi(y_t | x_{t - 1})$\n",
    "\n",
    "Could resample the particles before applying the kernel, but need to be careful when to compute the weights (Read De Moral et al. 2006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sticking the Landing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roeder, G., Wu, Y., & Duvenaud, D. (2017). Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference. In arXiv [stat.ML]. arXiv. http://arxiv.org/abs/1703.09194\n",
    "\n",
    "$ELBO \\triangleq \\mathbb{E}_{Q(z; \\phi)} \\left[ log P(x, z) - log Q(z; \\phi) \\right] \\le P(x)$\n",
    "\n",
    "We can compute gradients via reparameterization, given $z = f(\\epsilon; \\phi)$ where $\\epsilon$ is distributed under some fixed distribution, meaning that the gradients of ELBO are:\n",
    "\n",
    "$\\nabla_\\phi ELBO = \\mathbb{E}_\\epsilon \\left[ \\nabla_\\phi \\left[ log P(x, z) - log Q(z; \\phi) \\right] \\right]$\n",
    "\n",
    "Now, it turns out that while computing that derivative via autodiff, there are two pathways where the gradients flow from $\\phi$: one via $z = f(\\epsilon)$ and one via $\\log |det \\frac{\\partial f}{\\partial \\phi}|$. We can see that by computing the total derivative wrt $\\phi$, which has the two terms (indirect followed by direct):\n",
    "\n",
    "$\\frac{\\partial}{\\partial z} [\\log P(x, z) - \\log Q(z; \\phi)] \\frac{\\partial}{\\partial \\phi} [f(\\epsilon; \\phi)] - \\frac{\\partial}{\\partial \\phi} \\log Q(z; \\phi)$\n",
    "\n",
    "when the surrogate is perfect, the first term is zero everywhere, but the latter term is not, thus contributing some variance. It does have zero mean (we can show this by exchanging gradient and integral, the integral becoming 1), so we can try to remove it.\n",
    "\n",
    "We do so by stopping the gradient used to compute the log-probability of Q. Note that in principle this term could be beneficial when surrogate is bad.\n",
    "\n",
    "Note that this estimator is biased when the IWAE form is used, fixed in the doubly reparameterized gradient paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"180pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 180.34 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-184 176.34,-184 176.34,4 -4,4\"/>\n",
       "<!-- epsilon -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>epsilon</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"48.45\" cy=\"-162\" rx=\"36.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"48.45\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">epsilon</text>\n",
       "</g>\n",
       "<!-- z -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>z</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"61.45\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"61.45\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">z</text>\n",
       "</g>\n",
       "<!-- epsilon&#45;&gt;z -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>epsilon&#45;&gt;z</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M51.59,-144.05C53.03,-136.35 54.76,-127.03 56.37,-118.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.85,-118.75 58.24,-108.28 52.97,-117.47 59.85,-118.75\"/>\n",
       "</g>\n",
       "<!-- phi -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>phi</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"129.45\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"129.45\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">phi</text>\n",
       "</g>\n",
       "<!-- phi&#45;&gt;z -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>phi&#45;&gt;z</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M115.36,-146.5C105.88,-136.73 93.2,-123.69 82.53,-112.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85,-110.22 75.52,-105.49 79.98,-115.1 85,-110.22\"/>\n",
       "</g>\n",
       "<!-- Q(z; phi) -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Q(z; phi)</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"129.45\" cy=\"-18\" rx=\"42.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"129.45\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Q(z; phi)</text>\n",
       "</g>\n",
       "<!-- phi&#45;&gt;Q(z; phi) -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>phi&#45;&gt;Q(z; phi)</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M129.45,-143.87C129.45,-119.67 129.45,-75.21 129.45,-46.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132.95,-46.19 129.45,-36.19 125.95,-46.19 132.95,-46.19\"/>\n",
       "</g>\n",
       "<!-- P(z, x) -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>P(z, x)</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"34.45\" cy=\"-18\" rx=\"34.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34.45\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">P(z, x)</text>\n",
       "</g>\n",
       "<!-- z&#45;&gt;P(z, x) -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>z&#45;&gt;P(z, x)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M55.05,-72.41C51.93,-64.34 48.11,-54.43 44.61,-45.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47.85,-44.03 40.99,-35.96 41.32,-46.55 47.85,-44.03\"/>\n",
       "</g>\n",
       "<!-- z&#45;&gt;Q(z; phi) -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>z&#45;&gt;Q(z; phi)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M75.54,-74.5C84.55,-65.22 96.45,-52.97 106.78,-42.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-44.56 113.95,-34.95 104.47,-39.68 109.5,-44.56\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fab3da15d60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "dot = Digraph()\n",
    "\n",
    "dot.node('epsilon')\n",
    "dot.node('phi')\n",
    "dot.node('z')\n",
    "dot.node('P(z, x)')\n",
    "dot.node('Q(z; phi)')\n",
    "\n",
    "dot.edges([\n",
    "    ['epsilon', 'z'],\n",
    "    ['phi', 'z'],\n",
    "    ['z', 'Q(z; phi)'],\n",
    "    ['z', 'P(z, x)'],\n",
    "])\n",
    "dot.edge('phi', 'Q(z; phi)', style='dashed')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IWAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $Q(z_{1:k}; \\phi) = \\prod_{k=1}^K Q(z_k; \\phi)$, we can compute a closer bound to marginal likelihood as follows:\n",
    "\n",
    "$ELBO_{IWAE} = \\mathbb{E}_{Q(z_{1:k})} \\left[ \\log \\frac{1}{K} \\sum_k w_k \\right]$.\n",
    "\n",
    "where $w_k = \\frac{P(x, z_k)}{Q(z_k; \\phi)}$.\n",
    "\n",
    "By default, increasing the K increases the relative amount of noise going into Q. This is not just because Q's contribution decreases, the gradients going into Q will go to 0, but they need not be noisy.\n",
    "\n",
    "If $Q$ is reparameterizable, we can write $\\nabla_\\phi ELBO_{IWAE}$ as:\n",
    "\n",
    "$\\mathbb{E}_{\\epsilon_{1:k}} \\nabla_\\phi \\left[ \\log \\frac{1}{K} \\sum_k w_k \\right]$\n",
    "\n",
    "$\\mathbb{E}_{\\epsilon_{1:k}} \\left[ \\frac{K}{\\sum_k w_k} \\nabla_\\phi \\frac{1}{K} \\sum_k w_k \\right]$\n",
    "\n",
    "$\\mathbb{E}_{\\epsilon_{1:k}} \\left[ \\sum_k \\frac{w_k}{\\sum_k w_k} \\nabla_\\phi \\log w_k \\right]$\n",
    "\n",
    "Like with Sticking the Landing, we can expand $\\nabla_\\phi \\log w_k$ as:\n",
    "\n",
    "$\\frac{\\partial}{\\partial z_k} [\\log P(x, z_k) - \\log Q(z_k; \\phi)] \\frac{\\partial}{\\partial \\phi} [f(\\epsilon_k; \\phi)] - \\frac{\\partial}{\\partial \\phi} \\log Q(z_k; \\phi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIMCO / REBAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write $ELBO_{IWAE}$ as \n",
    "\n",
    "$\\mathbb{E}_{Q(z_{1:k})} \\left[ \\log \\frac{1}{K} \\sum_k w_k \\right]$\n",
    "\n",
    "where\n",
    "\n",
    "$w_k = \\frac{P(x, z_k)}{Q(z_k; \\phi)}$\n",
    "\n",
    "If we're using a REINFORCE estimator, we get:\n",
    "\n",
    "$\\mathbb{E}_{Q(z_{1:k})} \\left[ \\left( \\log \\frac{1}{K} \\sum_k w_k \\right) \\nabla Q(z_{1:k}) + \\nabla  \\log \\frac{1}{K} \\sum_k w_k \\right]$\n",
    "\n",
    "The first term is high variance, so we need some control variate.\n",
    "\n",
    "VIMCO uses a control variate based on leave-one-out geometric and arithmetic means.\n",
    "\n",
    "REBAR uses a relaxation as a control variate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $P(x; \\theta)$ want to compute gradients of $\\mathbb{E}_{P(x)} \\left[ g(x; \\theta) \\right]$.\n",
    "\n",
    "We can derive the following:\n",
    "\n",
    "$\\nabla \\int dx P(x) g(x) = $\n",
    "\n",
    "$\\int dx g(x) \\nabla P(x) + \\int dx P(x) \\nabla g(x)$\n",
    "\n",
    "$\\int dx g(x) P(x) \\nabla \\log P(x) + \\mathbb{E} [\\nabla g(x)]$\n",
    "\n",
    "$\\mathbb{E}_{P(x)} [g(x) \\nabla \\log P(x)] + \\mathbb{E}_{P(x)} [ \\nabla g(x)]$\n",
    "\n",
    "compare with reparameterization $x = f(\\epsilon)$:\n",
    "\n",
    "$\\mathbb{E}_{\\epsilon} [\\nabla g(f(\\epsilon))]$\n",
    "\n",
    "$\\mathbb{E}_{\\epsilon} [\\frac{\\partial}{\\partial x} g(f(\\epsilon)) \\frac{\\partial}{\\partial \\theta} f(\\epsilon) + \\frac{\\partial}{\\partial \\theta} g(f(\\epsilon))]$\n",
    "\n",
    "$\\mathbb{E}_{\\epsilon} [\\frac{\\partial}{\\partial x} g(f(\\epsilon)) \\frac{\\partial}{\\partial \\theta} f(\\epsilon)] + \\mathbb{E}_{P(x)} [ \\nabla g(x)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Weighted Wake Sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $P(x, z; \\theta)$ and $Q(z | x; \\phi)$, we can learn the parameters as follows:\n",
    "\n",
    "**Wake $\\theta$**\n",
    "\n",
    "Optimize $ELBO$. Can use IWAE too.\n",
    "\n",
    "**Sleep $\\phi$**\n",
    "\n",
    "Optimize $\\mathbb{E}_{P(x; \\theta)} \\left[ -D_{KL} [ P(z | x; \\theta) || Q(z | x; \\phi) ] \\right]$. The two expectations can be optimized by sampling from $P(x, z)$.\n",
    "\n",
    "**Wake $\\phi$**\n",
    "\n",
    "$\\mathbb{E}_{\\hat{P}(x)} \\left[ -D_{KL} [ P(z | x; \\theta) || Q(z | x; \\phi) ] \\right]$. Note that the first expectation now uses the data distribution. The second expectation can be estimated via a self-normalized importance sampler using $Q(z | x; \\phi)$ as the proposal. The self-normalization yields a biased estimator.\n",
    "\n",
    "$\\mathbb{E}_{z_{1:k}} \\left[ \\frac{w_k}{\\sum_k w_k} \\nabla_\\phi \\log Q(z | x; \\phi) \\right ]$\n",
    "\n",
    "Which can be reparameterized (see DReG below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doubly Reparameterized Gradient Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with IWAE gradient estimator:\n",
    "\n",
    "$\\mathbb{E}_{\\epsilon_{1:k}} \\left[ \\sum_k \\frac{w_k}{\\sum_k w_k} \\left(\\frac{\\partial}{\\partial z_k} \\log w_k \\frac{\\partial}{\\partial \\phi} f(\\epsilon_k; \\phi) - \\frac{\\partial}{\\partial \\phi} \\log Q(z_k; \\phi) \\right) \\right]$\n",
    "\n",
    "take the second term and write it as:\n",
    "\n",
    "$\\sum_k \\mathbb{E}_{z_{/k}} \\mathbb{E}_{z_{k}} \\left[ \\frac{w_k}{\\sum_k w_k} \\frac{\\partial}{\\partial \\phi} \\log Q(z_k; \\phi)) \\right ]$\n",
    "\n",
    "We can do this because $z$ is treated as constant because this is a partial derivative. Now the inner expectation looks like a REINFORCE term, so we replace it with its reparameterized version (see the REINFORCE section):\n",
    "\n",
    "$\\mathbb{E}_{z_{k}} \\left[ \\frac{w_k}{\\sum_k w_k} \\frac{\\partial}{\\partial \\phi} \\log Q(z_k; \\phi)) \\right ]$\n",
    "\n",
    "$\\mathbb{E}_{\\epsilon_{k}} \\left[ \\frac{\\partial}{\\partial z_k} \\frac{w_k}{\\sum_k w_k} \\frac{\\partial}{\\partial \\phi} f(\\epsilon_k) \\right ]$\n",
    "\n",
    "By quotient rule:\n",
    "\n",
    "$\\mathbb{E}_{\\epsilon_{k}} \\left[ \\left( \\frac{1}{\\sum_k w_k} - \\frac{w_k}{(\\sum_k w_k)^2} \\right) \\frac{\\partial}{\\partial z_k} w_k \\frac{\\partial}{\\partial \\phi} f(\\epsilon_k) \\right ]$\n",
    "\n",
    "$\\mathbb{E}_{\\epsilon_{k}} \\left[ \\left( \\frac{w_k}{\\sum_k w_k} - \\frac{w_k^2}{(\\sum_k w_k)^2} \\right) \\frac{\\partial}{\\partial z_k} \\log w_k \\frac{\\partial}{\\partial \\phi} f(\\epsilon_k) \\right ]$\n",
    "\n",
    "When K = 1, this term vanishes (weirdly, it vanishes no matter the Q? maybe not that weird, given that we knew this term was 0 in IWAE already)\n",
    "\n",
    "If we plug this into the original IWAE, we get:\n",
    "\n",
    "$\\mathbb{E}_{\\epsilon_{1:k}} \\left[ \\sum_k \\frac{w_k^2}{(\\sum_k w_k)^2} \\left(\\frac{\\partial}{\\partial z_k} \\log w_k \\frac{\\partial}{\\partial \\phi} f(\\epsilon_k; \\phi) \\right) \\right]$\n",
    "\n",
    "So you... square the weights and stop the gradient to the $\\log Q$ term I guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
